{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weekly report generator\n",
    "## WARNING: out of date with .py version\n",
    "Creates a folder and populates it with statistical plots based on jobs that started (and have now finished) for the week ending in today's date. Designed to be run by anyone, but may need to change some python libraries.\n",
    "\n",
    "Can be changed to specify a specific date.\n",
    "\n",
    "Plots made: master histogram, largest users, weekly node data flow rate, table of totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing epmt_query\n",
      "importing pandas\n"
     ]
    }
   ],
   "source": [
    "#import  epmt query \n",
    "print('importing epmt_query')\n",
    "import epmt_query as eq\n",
    "# import matplot for better plotting functions\n",
    "import sys\n",
    "sys.path.insert(0,'/home/USER/pip_experiment')   #required for matplotlib in my workstation env\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages     #allows for creation of multipage pdf\n",
    "import numpy as np\n",
    "# import pandas. optional but helpful 'display.max_columns' arg shows all DataFrame columns when printing\n",
    "print('importing pandas')\n",
    "import os\n",
    "import pandas\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "#for percentile intepolation\n",
    "import scipy.stats as stats\n",
    "\n",
    "import time\n",
    "import pickle   #to load in metrics history\n",
    "pandas.set_option('display.max_columns', None)\n",
    "#number of days behind recorder is. Important to let jobs finish\n",
    "off_set_days = 4    #number of days delayed the query is. This allows for job completion and an easier query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at  1.0 % completion. time elapsed: 0.006671762466430664\n",
      "at  2.0 % completion. time elapsed: 0.010897254943847657\n",
      "at  3.0 % completion. time elapsed: 0.014671079317728679\n",
      "at  4.0 % completion. time elapsed: 0.018772971630096436\n",
      "at  5.0 % completion. time elapsed: 0.024956937630971274\n",
      "at  6.0 % completion. time elapsed: 0.029293715953826904\n",
      "at  7.0 % completion. time elapsed: 0.033765900135040286\n",
      "at  8.0 % completion. time elapsed: 0.038131332397460936\n",
      "at  9.0 % completion. time elapsed: 0.04244678815205892\n",
      "at  10.0 % completion. time elapsed: 0.04702955881754557\n",
      "at  11.0 % completion. time elapsed: 0.05132550001144409\n",
      "at  12.0 % completion. time elapsed: 0.055569394429524736\n",
      "at  13.0 % completion. time elapsed: 0.06372673908869425\n",
      "at  14.0 % completion. time elapsed: 0.06811550855636597\n",
      "at  15.0 % completion. time elapsed: 0.07336271603902181\n",
      "at  16.0 % completion. time elapsed: 0.078497314453125\n",
      "at  17.0 % completion. time elapsed: 0.08311345179875691\n",
      "at  18.0 % completion. time elapsed: 0.08720496495564779\n",
      "at  19.0 % completion. time elapsed: 0.09148024717966716\n",
      "at  20.0 % completion. time elapsed: 0.09566752910614014\n",
      "at  21.0 % completion. time elapsed: 0.09964913924535115\n",
      "at  22.0 % completion. time elapsed: 0.10288297335306804\n",
      "at  23.0 % completion. time elapsed: 0.10610920588175456\n",
      "at  24.0 % completion. time elapsed: 0.10947221120198568\n",
      "at  25.0 % completion. time elapsed: 0.1128794272740682\n",
      "at  26.0 % completion. time elapsed: 0.11798421144485474\n",
      "at  27.0 % completion. time elapsed: 0.12147486209869385\n",
      "at  28.0 % completion. time elapsed: 0.12479121685028076\n",
      "at  29.0 % completion. time elapsed: 0.1281602382659912\n",
      "at  30.0 % completion. time elapsed: 0.13152757088343303\n",
      "at  31.0 % completion. time elapsed: 0.1349617044130961\n",
      "at  32.0 % completion. time elapsed: 0.13831350008646648\n",
      "at  33.0 % completion. time elapsed: 0.14178876082102457\n",
      "at  34.0 % completion. time elapsed: 0.14522942304611205\n",
      "at  35.0 % completion. time elapsed: 0.14868627389272054\n",
      "at  36.0 % completion. time elapsed: 0.15212654670079548\n",
      "at  37.0 % completion. time elapsed: 0.15549043814341226\n",
      "at  38.0 % completion. time elapsed: 0.15892548163731893\n",
      "at  39.0 % completion. time elapsed: 0.16429471174875895\n",
      "at  40.0 % completion. time elapsed: 0.1676836570103963\n",
      "at  41.0 % completion. time elapsed: 0.17112526098887126\n",
      "at  42.0 % completion. time elapsed: 0.17478403250376384\n",
      "at  43.0 % completion. time elapsed: 0.17834078073501586\n",
      "at  44.0 % completion. time elapsed: 0.1818146308263143\n",
      "at  45.0 % completion. time elapsed: 0.1855470339457194\n",
      "at  46.0 % completion. time elapsed: 0.1888250192006429\n",
      "at  47.0 % completion. time elapsed: 0.19225252866744996\n",
      "at  48.0 % completion. time elapsed: 0.19568177461624145\n",
      "at  49.0 % completion. time elapsed: 0.19915942351023355\n",
      "at  50.0 % completion. time elapsed: 0.20267916520436605\n",
      "at  51.0 % completion. time elapsed: 0.20621301333109537\n",
      "at  52.0 % completion. time elapsed: 0.20983206033706664\n",
      "at  53.0 % completion. time elapsed: 0.21351211071014403\n",
      "at  54.0 % completion. time elapsed: 0.2196585734685262\n",
      "at  55.0 % completion. time elapsed: 0.22311317920684814\n",
      "at  56.0 % completion. time elapsed: 0.2268138607343038\n",
      "at  57.0 % completion. time elapsed: 0.23062996864318847\n",
      "at  58.0 % completion. time elapsed: 0.23440927267074585\n",
      "at  59.0 % completion. time elapsed: 0.23796311616897584\n",
      "at  60.0 % completion. time elapsed: 0.24160704215367634\n",
      "at  61.0 % completion. time elapsed: 0.2452012300491333\n",
      "at  62.0 % completion. time elapsed: 0.24899887641270954\n",
      "at  63.0 % completion. time elapsed: 0.25247164567311603\n",
      "at  64.0 % completion. time elapsed: 0.255982780456543\n",
      "at  65.0 % completion. time elapsed: 0.25953800678253175\n",
      "at  66.0 % completion. time elapsed: 0.2631166934967041\n",
      "at  67.0 % completion. time elapsed: 0.26699163516362506\n",
      "at  68.0 % completion. time elapsed: 0.2704596479733785\n",
      "at  69.0 % completion. time elapsed: 0.27403748432795205\n",
      "at  70.0 % completion. time elapsed: 0.27816951274871826\n",
      "at  71.0 % completion. time elapsed: 0.2851859927177429\n",
      "at  72.0 % completion. time elapsed: 0.289346182346344\n",
      "at  73.0 % completion. time elapsed: 0.2938794175783793\n",
      "at  74.0 % completion. time elapsed: 0.29825040499369304\n",
      "at  75.0 % completion. time elapsed: 0.3087235609690348\n",
      "at  76.0 % completion. time elapsed: 0.31820963621139525\n",
      "at  77.0 % completion. time elapsed: 0.32650007009506227\n",
      "at  78.0 % completion. time elapsed: 0.33489058415095013\n",
      "at  79.0 % completion. time elapsed: 0.34740233421325684\n",
      "at  80.0 % completion. time elapsed: 0.3582009434700012\n",
      "at  81.0 % completion. time elapsed: 0.3686382492383321\n",
      "at  82.0 % completion. time elapsed: 0.3772465904553731\n",
      "at  83.0 % completion. time elapsed: 0.38524550596872964\n",
      "at  84.0 % completion. time elapsed: 0.39327998956044513\n",
      "at  85.0 % completion. time elapsed: 0.40322314500808715\n",
      "at  86.0 % completion. time elapsed: 0.4136360724767049\n",
      "at  87.0 % completion. time elapsed: 0.42279934883117676\n",
      "at  88.0 % completion. time elapsed: 0.43390816847483316\n",
      "at  89.0 % completion. time elapsed: 0.4429431517918905\n",
      "at  90.0 % completion. time elapsed: 0.4511715849240621\n",
      "at  91.0 % completion. time elapsed: 0.45945701201756795\n",
      "at  92.0 % completion. time elapsed: 0.4675908088684082\n",
      "at  93.0 % completion. time elapsed: 0.4756981412569682\n",
      "at  94.0 % completion. time elapsed: 0.4839144468307495\n",
      "at  95.0 % completion. time elapsed: 0.4919277270634969\n",
      "at  96.0 % completion. time elapsed: 0.4999140977859497\n",
      "at  97.0 % completion. time elapsed: 0.5079793890317281\n",
      "at  98.0 % completion. time elapsed: 0.5182322820027669\n",
      "at  99.0 % completion. time elapsed: 0.5285642743110657\n",
      "at  100.0 % completion. time elapsed: 0.5380539377530416\n",
      "jobs in this time period:  37400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#check to see if day is already recorded. if it is, exit code\n",
    "#this allows code to be run much more frequentky, without redoing days, which helps whith timing out issues\n",
    "recent_job = eq.get_jobs(limit = 1, after = -1*off_set_days, fmt = 'orm')[0]\n",
    "\n",
    "recent_job_date = recent_job.created_at\n",
    "current_date = str(recent_job_date.strftime(\"%m\")+'-'+recent_job_date.strftime(\"%d\")+'-'+recent_job_date.strftime(\"%y\"))\n",
    "filename = 'weekly_metric_history_DO_NOT_DELETE.pkl'\n",
    "# Read dictionary pkl file\n",
    "with open(filename, 'rb') as fp:\n",
    "    metrics_history = pickle.load(fp)\n",
    "#if current_date in metrics_history['date']:\n",
    "#    sys.exit(['date already recorded'])\n",
    "    \n",
    "\n",
    "#query for data of the last week from exactly this moment\n",
    "start = time.time()\n",
    "x = datetime.datetime.now() - timedelta(off_set_days)\n",
    "limiter = eq.get_jobs(after=-9, before =-1*off_set_days, fmt = 'orm')  #to speed up query, we use orm to count number of jobs in the week and use this as our limit\n",
    "job_num = limiter.count()\n",
    "all_jobs = []\n",
    "loop_number = 100#How many parts the query is broken into. Small queries run faster.\n",
    "for aa in range(loop_number):\n",
    "    temp_jobs = eq.get_jobs(limit =int(job_num/loop_number), after = (-7-off_set_days), before = -1*off_set_days, offset = int(job_num*aa/loop_number), fmt = 'dict' , trigger_post_process = False)\n",
    "    all_jobs.extend(temp_jobs)\n",
    "    print('at ',(aa+1)*100/loop_number,'% completion. time elapsed:',(time.time()-start)/60)\n",
    "print('jobs in this time period: ', len(all_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#backup option for job query. do query for a specific week\\nstart_date = datetime.datetime(2024, 4, 17, 23, 00)    #day that ends the week\\nolder_date = start_date + datetime.timedelta(days =-7) \\nx = start_date\\n\\nstart = time.time()\\nolder_orm = eq.get_jobs(after=older_date, fmt = 'orm')\\nstart_orm = eq.get_jobs(after=start_date, fmt = 'orm')\\njob_num = older_orm.count() - start_orm.count()    #clip off the jobs that happened between current time and the end of the week being reported on\\nall_jobs = []\\nloop_number = 100   #How many parts the query is broken into. Small queries run faster.\\nfor aa in range(loop_number):\\n    temp_jobs = eq.get_jobs(limit =int(job_num/loop_number), offset = int(job_num*aa/loop_number) + start_orm.count(), fmt = 'dict', trigger_post_process = False)\\n    all_jobs.extend(temp_jobs)\\n    print('at ',(aa+1)*100/loop_number,'% completion. time elapsed:',(time.time()-start)/60)\\nprint('jobs in this time period: ', len(all_jobs))\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#backup option for job query. do query for a specific week\n",
    "start_date = datetime.datetime(2024, 4, 17, 23, 00)    #day that ends the week\n",
    "older_date = start_date + datetime.timedelta(days =-7) \n",
    "x = start_date\n",
    "\n",
    "start = time.time()\n",
    "older_orm = eq.get_jobs(after=older_date, fmt = 'orm')\n",
    "start_orm = eq.get_jobs(after=start_date, fmt = 'orm')\n",
    "job_num = older_orm.count() - start_orm.count()    #clip off the jobs that happened between current time and the end of the week being reported on\n",
    "all_jobs = []\n",
    "loop_number = 100   #How many parts the query is broken into. Small queries run faster.\n",
    "for aa in range(loop_number):\n",
    "    temp_jobs = eq.get_jobs(limit =int(job_num/loop_number), offset = int(job_num*aa/loop_number) + start_orm.count(), fmt = 'dict', trigger_post_process = False)\n",
    "    all_jobs.extend(temp_jobs)\n",
    "    print('at ',(aa+1)*100/loop_number,'% completion. time elapsed:',(time.time()-start)/60)\n",
    "print('jobs in this time period: ', len(all_jobs))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#find today's date and create folder with that date in a fashion usable by folders\n",
    "chosen_date = str(x.strftime(\"%m\")+'-'+x.strftime(\"%d\")+'-'+x.strftime(\"%y\"))\n",
    "if not os.path.isdir('report_directory/weekly_report_'+chosen_date):\n",
    "    os.makedirs('report_directory/weekly_report_'+chosen_date+'/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make plots\n",
    "with setup done, we will now produce a series of plots and data to save out to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 % at  0.013331302007039388 minutes\n",
      "5 % at  0.013369810581207276 minutes\n",
      "8 % at  0.013406205177307128 minutes\n",
      "10 % at  0.013438725471496582 minutes\n",
      "13 % at  0.014128005504608155 minutes\n",
      "16 % at  0.09013629754384359 minutes\n",
      "18 % at  31.936073219776155 minutes\n",
      "21 % at  107.69020578463872 minutes\n"
     ]
    }
   ],
   "source": [
    "###Master histogram is a 3x9 plot of subplots, with each plot being a histogram of a specific epmt metric, plotted with 200 bins, and on a log scale\n",
    "#Generates lists for all variables that can be used to make master histogram and totals table\n",
    "#Construct dictionaries\n",
    "start = time.time()\n",
    "metric_list = [ 'duration', 'rchar', 'syscr', 'syscw', 'wchar', 'cstime', 'cutime', 'majflt', 'cpu_time', 'minflt', 'rssmax', 'cmajflt','cminflt', 'inblock', 'outblock', 'usertime', 'num_procs', 'starttime', 'vol_ctxsw', 'read_bytes', 'systemtime', 'time_oncpu', 'timeslices', 'invol_ctxsw', 'write_bytes', 'time_waiting', 'cancelled_write_bytes']\n",
    "metric_dict = {}\n",
    "for i in metric_list:\n",
    "    metric_dict[i] = []   #Use metric names as keys for new dictionary for searching epmt dictionary\n",
    "#Sort into usbale variables in one for loop\n",
    "fig_master, ax = plt.subplots(nrows=3,ncols=9,figsize=(54,20))\n",
    "ax =ax.ravel()    #Matplotlib command that is mandatory and mysterious\n",
    "for ff in range(len(metric_list)):\n",
    "    metric = metric_list[ff]\n",
    "    for job_instance in range(len(all_jobs)):\n",
    "        #to track progress of secondary post processing for neccesary jobs\n",
    "        if (job_instance+1) % 1000 == 0 and metric == 'rchar':\n",
    "            print(int(job_instance/len(all_jobs)*100), '% at ', (time.time()-start)/60, 'minutes')\n",
    "        if all_jobs[job_instance].get(metric) == None:    #If job does not have all data, it finds that job and processes it through a new query\n",
    "            try:\n",
    "                all_jobs[job_instance] = eq.get_jobs(jobs = all_jobs[job_instance]['jobid'], fmt = 'dict',  trigger_post_process = True)[0]  \n",
    "            except:\n",
    "                continue\n",
    "        #double check secondary query was succesful\n",
    "        if all_jobs[job_instance].get(metric) == None:\n",
    "            continue\n",
    "        if all_jobs[job_instance].get(metric) != None:   #Prevents breakage if set is empty or doesn't exist\n",
    "            metric_dict[metric].append(all_jobs[job_instance][metric])\n",
    "    #Make plot\n",
    "    plt.style.use('default')   #Resets plot style to normal, avoiding any local environment settings that may mess with plot\n",
    "\n",
    "    bins = 200\n",
    "    #Bin check for zero, as it messes up np.log10()\n",
    "    if min(metric_dict[metric]) == 0:\n",
    "        bottom_bin = 0\n",
    "    else:\n",
    "        bottom_bin = np.log10(min(metric_dict[metric]))\n",
    "    #Set scaling metric\n",
    "    #Makes master histogram\n",
    "    ax[ff].hist(metric_dict[metric], bins = np.logspace(bottom_bin,np.log10(max(metric_dict[metric])),bins), color = 'orange')\n",
    "    ax[ff].set_xlabel(metric, fontsize = 20)\n",
    "    ax[ff].set_xscale('log')\n",
    "ax[0].set_ylabel('counts', fontsize = 20)\n",
    "ax[9].set_ylabel('counts', fontsize = 20)\n",
    "ax[18].set_ylabel('counts', fontsize = 20)\n",
    "plt.suptitle('Master Histogram '+chosen_date, fontsize = 60)\n",
    "plt.savefig('report_directory/weekly_report_'+chosen_date+'/master_hist_'+chosen_date+'.pdf', bbox_inches='tight', format = 'pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(all_jobs[job_instance]['jobid'])\n",
    "eq.get_jobs(jobs = [all_jobs[job_instance]['jobid']], fmt = 'dict',  trigger_post_process = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate weekly user ranking. Ranking based on rssmax and CPU time. Piechart by CPU time \n",
    "users = []\n",
    "for job in all_jobs:\n",
    "    users.append(job['user'])\n",
    "users = set(users)   #creates a list of every unique user name\n",
    "totals = {}\n",
    "scores = []\n",
    "final_score = {}\n",
    "for person in users:\n",
    "    totals[person] = [0,0,0]   #rssmax, cpu_time, number of jobs\n",
    "    final_score[person] = 0\n",
    "#sort data into each person. we care about rssmax, cpu_time, and total number of jobs\n",
    "for job in all_jobs:\n",
    "    if job.get('rssmax'):\n",
    "        totals[job['user']][0] += job['rssmax']\n",
    "        totals[job['user']][1] += job['cpu_time']\n",
    "        totals[job['user']][2] += 1\n",
    "for person in users:\n",
    "    if totals[person][2] == 0:    #edge case in which user has corrupted jobs\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        totals[person][0] = totals[person][0]/totals[person][2]   #average rssmax\n",
    "        totals[person][1] = totals[person][1]                     #cpu_time\n",
    "        scores.append([person,totals[person][0],totals[person][1], totals[person][2]])\n",
    "#sort rankings\n",
    "num_jobs_ranking = sorted(scores, key=lambda score: score[3], reverse=True)\n",
    "num_jobs_ranking = num_jobs_ranking[0:int(.7*len(num_jobs_ranking))]   #elimate the bottom 70% based on jobs run\n",
    "rssmax_ranking = sorted(num_jobs_ranking, key=lambda score: score[1], reverse=True)\n",
    "cpu_time_ranking = sorted(num_jobs_ranking, key=lambda score: score[2], reverse=True)\n",
    "#tally final score based on ranking of average rssmax and cpu_time  totalled\n",
    "final_score = {}\n",
    "for aa in range(len(num_jobs_ranking)):   #make keys fro score dictionary\n",
    "    final_score[num_jobs_ranking[aa][0]] = 0\n",
    "for count in range(len(num_jobs_ranking)):   #combine score\n",
    "    final_score[rssmax_ranking[count][0]] += count\n",
    "    final_score[cpu_time_ranking[count][0]] += count\n",
    "#wrap into list because dictionaries do not sort easily\n",
    "final_rank = []\n",
    "for bb in range(len(num_jobs_ranking)):   #make keys fro score dictionary\n",
    "    final_rank.append([final_score[num_jobs_ranking[bb][0]], num_jobs_ranking[bb][0]])\n",
    "final_rank = sorted(final_rank) #top users at beginning of list \n",
    "\n",
    "# Data to plot\n",
    "num_users = 10   #number of users that appears in chart\n",
    "labels = []\n",
    "sizes = []\n",
    "#creat lists with order based on final score\n",
    "for aa in range(num_users):\n",
    "    name = final_rank[aa][1]\n",
    "    labels.append(str(name+', # jobs = '+str(totals[name][2])))\n",
    "    sizes.append(totals[name][1])\n",
    "\n",
    "# Plot\n",
    "fig_users = plt.figure(figsize=(7,7))\n",
    "plt.style.use('default')\n",
    "plt.pie(sizes)\n",
    "plt.axis('equal')\n",
    "plt.title('top ' +str(num_users)+' users (CPU_time displayed) for ' + chosen_date)\n",
    "plt.legend(labels=labels, loc = 'lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('report_directory/weekly_report_'+chosen_date+'/user_ranking_'+chosen_date+'.pdf', bbox_inches='tight', format = 'pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###plot of data flow rate for each node. data flow rate = write&read bytes/cpu_time. It is an aproximate of node health, as it represents the rate of data the node is going through.\n",
    "#Pull data from jobs and store in dictionaries\n",
    "#Make list of unique nodes\n",
    "node_list = [] \n",
    "for jobs_instance in range(len(all_jobs)):\n",
    "    node_list.append(all_jobs[jobs_instance]['env_dict']['SLURM_NODELIST'])\n",
    "node_list = sorted(set(node_list))\n",
    "#Make dictionary with each unique name as a key\n",
    "CPU_runtime_dict = {}\n",
    "wr_bytes_dict = {}\n",
    "rd_bytes_dict = {}\n",
    "flow_avg_dict = {}\n",
    "#Each tag group has its own key in variable dictonary\n",
    "for dict_key in node_list:   \n",
    "    CPU_runtime_dict[dict_key] = []\n",
    "    wr_bytes_dict[dict_key] = []\n",
    "    rd_bytes_dict[dict_key] = []\n",
    "    flow_avg_dict[dict_key] = []\n",
    "#Now we can run through evry job and grab its specific stats to attach to its node\n",
    "for jobs_instance in range(len(all_jobs)):   \n",
    "    dict_key_instance = all_jobs[jobs_instance]['env_dict']['SLURM_NODELIST']\n",
    "    if all_jobs[jobs_instance].get('read_bytes') is not None:\n",
    "        if all_jobs[jobs_instance]['read_bytes'] + all_jobs[jobs_instance]['write_bytes'] > 0:   #if statements to adress problem cases\n",
    "            CPU_runtime_dict[dict_key_instance].append(all_jobs[jobs_instance]['cpu_time'])\n",
    "            wr_bytes_dict[dict_key_instance].append(all_jobs[jobs_instance]['write_bytes'])\n",
    "            rd_bytes_dict[dict_key_instance].append(all_jobs[jobs_instance]['read_bytes'])\n",
    "            flow_avg_dict[dict_key_instance].append((all_jobs[jobs_instance]['write_bytes']+all_jobs[jobs_instance]['read_bytes'])/all_jobs[jobs_instance]['cpu_time'])\n",
    "\n",
    "#Calculations Section\n",
    "#We separate nodes by type, as an nodes do different things than pp nodes\n",
    "flow_avg = []\n",
    "flow_avg_pp = []\n",
    "flow_err_pp = []\n",
    "flow_error = []\n",
    "pp_nodes = []\n",
    "for node in node_list:\n",
    "    if node.find('an') == -1:\n",
    "        pp_nodes.append(node)\n",
    "#Average and error calculations for each node\n",
    "for node_name in node_list:\n",
    "    if node_name in pp_nodes:\n",
    "        flow_avg_pp.append(sum(flow_avg_dict[node_name])/len(flow_avg_dict[node_name]))\n",
    "        flow_err_pp.append(np.std(flow_avg_dict[node_name])/(len(flow_avg_dict[node_name]))**.5)\n",
    "#Plot creation for rssmax of nodes\n",
    "pp_nodes_short = []   #Shorten node name by removing letters. this makes plot slighly cleaer\n",
    "for aa in range(len(pp_nodes)):\n",
    "    pp_nodes_short.append(pp_nodes[aa][2:5])\n",
    "#setup dictionaries\n",
    "separated_node_data = {}\n",
    "separated_node_data['zero'] = [],[],[]   #[node name], [value], [error]\n",
    "separated_node_data['one_two'] = [],[],[]\n",
    "separated_node_data['three'] = [],[],[]\n",
    "node_categories = ['zero','one_two','three']\n",
    "#separate nodes by first number, as that correlates to hardware\n",
    "for ii in range(len(pp_nodes_short)):\n",
    "    node = pp_nodes_short[ii]\n",
    "    if node[0] == '0':\n",
    "        separated_node_data['zero'][0].append(pp_nodes_short[ii])\n",
    "        separated_node_data['zero'][1].append(flow_avg_pp[ii])\n",
    "        separated_node_data['zero'][2].append(flow_err_pp[ii])\n",
    "    if node[0] == '1' or node[0] == '2':\n",
    "        separated_node_data['one_two'][0].append(pp_nodes_short[ii])\n",
    "        separated_node_data['one_two'][1].append(flow_avg_pp[ii])\n",
    "        separated_node_data['one_two'][2].append(flow_err_pp[ii])\n",
    "    if node[0] == '3':\n",
    "        separated_node_data['three'][0].append(pp_nodes_short[ii])\n",
    "        separated_node_data['three'][1].append(flow_avg_pp[ii])\n",
    "        separated_node_data['three'][2].append(flow_err_pp[ii])\n",
    "##################################################################################\n",
    "#subsection: call historical data for each node and calculate the values needed to display with errorbars\n",
    "#add in percentiles row. need to call history and find percentile of weeks metric\n",
    "filename = '../hpc_health_report/weekly_node_flow_history_DO_NOT_DELETE.pkl'\n",
    "# Read dictionary pkl file\n",
    "with open(filename, 'rb') as fp:\n",
    "    node_history = pickle.load(fp)\n",
    "del node_history['date']     #remove dates as to leave only variables that are being used\n",
    "#manipulate the historical dat flow rate for each node into somwthing usable\n",
    "#Average and error calculations for each node\n",
    "historical_flow_avg_pp = []\n",
    "historical_flow_err_pp = []\n",
    "\n",
    "#due to nodes being added in the future this code block exists as an edge case.\n",
    "bad_nodes = []\n",
    "for node in pp_nodes_short:\n",
    "    if node not in node_history.keys():\n",
    "        bad_nodes.append(node)\n",
    "for node in bad_nodes:\n",
    "    pp_nodes_short.remove(node)\n",
    "\n",
    "#extract historical data\n",
    "for node_name in pp_nodes_short:\n",
    "    node_history[node_name] = [x for x in node_history[node_name] if x is not None] \n",
    "    historical_flow_avg_pp.append(sum(node_history[node_name])/len(node_history[node_name]))\n",
    "    historical_flow_err_pp.append(np.std(node_history[node_name])/(len(node_history[node_name]))**.5)\n",
    "\n",
    "\n",
    "#setup dictionaries\n",
    "historical_separated_node_data = {}\n",
    "historical_separated_node_data['zero'] = [],[],[]   #[node name], [value], [error]\n",
    "historical_separated_node_data['one_two'] = [],[],[]\n",
    "historical_separated_node_data['three'] = [],[],[]\n",
    "node_categories = ['zero','one_two','three']\n",
    "#separate nodes by first number, as that correlates to hardware\n",
    "for ii in range(len(pp_nodes_short)):\n",
    "    node = pp_nodes_short[ii]\n",
    "    if node in node_history.keys():\n",
    "        if node[0] == '0':\n",
    "            historical_separated_node_data['zero'][0].append(pp_nodes_short[ii])\n",
    "            historical_separated_node_data['zero'][1].append(historical_flow_avg_pp[ii])\n",
    "            historical_separated_node_data['zero'][2].append(historical_flow_err_pp[ii])\n",
    "        if node[0] == '1' or node[0] == '2':\n",
    "            historical_separated_node_data['one_two'][0].append(pp_nodes_short[ii])\n",
    "            historical_separated_node_data['one_two'][1].append(historical_flow_avg_pp[ii])\n",
    "            historical_separated_node_data['one_two'][2].append(historical_flow_err_pp[ii])\n",
    "        if node[0] == '3':\n",
    "            historical_separated_node_data['three'][0].append(pp_nodes_short[ii])\n",
    "            historical_separated_node_data['three'][1].append(historical_flow_avg_pp[ii])\n",
    "            historical_separated_node_data['three'][2].append(historical_flow_err_pp[ii])\n",
    "##################################################################################\n",
    "#make subplots breaking up long plot\n",
    "plt.style.use('default')\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1,figsize=(18,9), sharex = 'none', sharey = 'all', gridspec_kw={'hspace': .35,'wspace':.05})\n",
    "ax =ax.ravel()\n",
    "plt.suptitle('average data flow rate ((read+write bytes)/CPU time) for PP nodes', fontweight ='bold', fontsize = 20)\n",
    "\n",
    "for aa in range(3):\n",
    "    node_key = node_categories[aa]\n",
    "    ax[aa].bar(separated_node_data[node_key][0],separated_node_data[node_key][1], yerr = separated_node_data[node_key][2], color = 'darkorchid', label = 'this week average + errorbar')\n",
    "    ax[aa].set_xticklabels(separated_node_data[node_key][0], rotation=90)\n",
    "    ax[aa].errorbar(historical_separated_node_data[node_key][0],historical_separated_node_data[node_key][1],yerr = historical_separated_node_data[node_key][2], color = 'red',fmt=\"o\", markersize = 3, label = 'all time average + errorbar')\n",
    "    ax[aa].set_ylabel('average data flow rate', fontsize = 15)\n",
    "#bells n whistles\n",
    "plt.xlabel('pp Job node', fontsize = 15)\n",
    "ax[0].legend()\n",
    "\n",
    "plt.tight_layout\n",
    "plt.show()\n",
    "plt.savefig('report_directory/weekly_report_'+chosen_date+'/node_flow_'+chosen_date+'.pdf', bbox_inches='tight', format = 'pdf')\n",
    "\n",
    "#lastly save data to a pkl file\n",
    "z = 0\n",
    "nodes_dictionary = {}\n",
    "for node in pp_nodes_short:\n",
    "    nodes_dictionary[node] = [flow_avg_pp[z]]\n",
    "    z += 1\n",
    "\n",
    "\n",
    "#eventually will use data to predict relative health of varous nodes. But first We need to build up those records\n",
    "filename = 'weekly_node_flow_history_DO_NOT_DELETE.pkl'\n",
    "# Read dictionary pkl file\n",
    "with open(filename, 'rb') as fp:\n",
    "    nodes_history = pickle.load(fp)\n",
    "    \n",
    "\n",
    "#record new dates\n",
    "if chosen_date not in nodes_history['date']:\n",
    "    for node in pp_nodes_short:\n",
    "        nodes_history[node].append(nodes_dictionary[node][0])\n",
    "    for node in bad_nodes:\n",
    "        nodes_history[node] = (len(node_history['016'])-1)*[None]   #if node does not exist in list, then give it all zeros with an established\n",
    "        nodes_history[node].append((sum(flow_avg_dict[f'pp{node}'])/len(flow_avg_dict[f'pp{node}'])))\n",
    "        \n",
    "    nodes_history['date'].append(chosen_date)\n",
    "# save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(nodes_history, fp)\n",
    "    print('nodes dictionary saved successfully to file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#This code is dark magic. Touch at your own risk.\n",
    "#Create table of values\n",
    "summed_metric_list = [ 'duration', 'rchar', 'syscr', 'syscw', 'wchar', 'cstime', 'cutime', 'majflt', 'cpu_time', 'minflt', 'rssmax', 'cmajflt','cminflt', 'usertime', 'num_procs', 'vol_ctxsw', 'read_bytes', 'systemtime', 'time_oncpu', 'timeslices', 'invol_ctxsw', 'write_bytes', 'time_waiting', 'cancelled_write_bytes']\n",
    "summed_metric_dict = {}\n",
    "for i in summed_metric_list:\n",
    "    summed_metric_dict[i] = [\"{:e}\".format(sum(metric_dict[i])),\"{:e}\".format(sum(metric_dict[i])/len(all_jobs)), 'TBD']   #change formating to scientific notation\n",
    "    \n",
    "#Add in missing metrics by hand\n",
    "flow = (sum(metric_dict['read_bytes'])+sum(metric_dict['write_bytes']))/sum(metric_dict['cpu_time'])\n",
    "summed_metric_dict['data_flow_rate'] = [\"{:e}\".format(flow),\"{:e}\".format(flow/len(all_jobs)),'TBD']\n",
    "summed_metric_dict['jobs'] = [\"{:e}\".format(len(all_jobs)),1,'TBD']\n",
    "summed_metric_list.append('data_flow_rate')\n",
    "summed_metric_list.append('jobs')\n",
    "\n",
    "#add in percentiles row. need to call history and find percentile of weeks metric\n",
    "filename = 'weekly_metric_history_DO_NOT_DELETE.pkl'\n",
    "# Read dictionary pkl file\n",
    "with open(filename, 'rb') as fp:\n",
    "    metrics_history = pickle.load(fp)\n",
    "specific_metric_history = metrics_history\n",
    "\n",
    "for i in summed_metric_list:\n",
    "    current_week_metric_percentile = stats.percentileofscore(np.array(metrics_history[i],dtype = float),float(summed_metric_dict[i][0]))\n",
    "    if current_week_metric_percentile <= 5:\n",
    "        summed_metric_dict[i][2] = 'VERY LOW'\n",
    "    if 5 < current_week_metric_percentile <= 25:\n",
    "        summed_metric_dict[i][2] = 'Low'    \n",
    "    if 25 < current_week_metric_percentile <= 75:\n",
    "        summed_metric_dict[i][2] = 'Average'\n",
    "    if 75 < current_week_metric_percentile <= 95:\n",
    "        summed_metric_dict[i][2] = 'High'\n",
    "    if 95 < current_week_metric_percentile:\n",
    "        summed_metric_dict[i][2] = 'VERY HIGH'\n",
    "    \n",
    "#some metrics need to have relative column based on averages instead total. we replace those values here\n",
    "average_metric_list = ['rssmax']\n",
    "for i in average_metric_list:\n",
    "    current_week_metric_percentile = stats.percentileofscore(np.array(metrics_history[i],dtype = float)/np.array(metrics_history['jobs'],dtype = float),float(summed_metric_dict[i][1]))\n",
    "    if current_week_metric_percentile <= 5:\n",
    "        summed_metric_dict[i][2] = 'VERY LOW'\n",
    "    if 5 < current_week_metric_percentile <= 25:\n",
    "        summed_metric_dict[i][2] = 'Low'    \n",
    "    if 25 < current_week_metric_percentile <= 75:\n",
    "        summed_metric_dict[i][2] = 'Average'\n",
    "    if 75 < current_week_metric_percentile <= 95:\n",
    "        summed_metric_dict[i][2] = 'High'\n",
    "    if 95 < current_week_metric_percentile:\n",
    "        summed_metric_dict[i][2] = 'VERY HIGH'\n",
    "\n",
    "footer_text = chosen_date  #goes in the corner. currently not in use\n",
    "data= []\n",
    "for key in summed_metric_list:\n",
    "    data.append(summed_metric_dict[key])\n",
    "    \n",
    "#Redfine metric list by hand to add units\n",
    "metric_unit_list = [ 'duration', 'rchar (Bytes)', 'syscr (Read syscalls)', 'syscw (write syscalls)', 'wchar (Bytes)', 'cstime', 'cutime', 'majflt', 'cpu_time (Nanoseconds)', 'minflt', 'rssmax (Kb)', 'cmajflt','cminflt', 'usertime (Microsecond)', 'num_procs', 'vol_ctxsw', 'read_bytes (bytes)', 'systemtime (Microsecond)', 'time_oncpu (nanoseconds)', 'timeslices (Run Periods)', 'invol_ctxsw', 'write_bytes (bytes)', 'time_waiting (nanoseconds)', 'cancelled_write_bytes (bytes)','data_flow_rate (kb/nanosecond)', 'Jobs']\n",
    "column_headers = ['Total', 'Average Per Job', 'Relative Size']\n",
    "row_headers = metric_unit_list\n",
    "#Table data needs to be non-numeric text\n",
    "cell_text = []\n",
    "for row in data:\n",
    "    cell_text.append([f'{x}' for x in row])\n",
    "#Get some lists of color specs for row and column headers\n",
    "rcolors = plt.cm.BuPu(np.full(len(row_headers), 0.1))\n",
    "ccolors = plt.cm.BuPu(np.full(len(column_headers), 0.1))\n",
    "#Create the figure. Setting a small pad on tight_layout\n",
    "#Seems to better regulate white space. Sometimes experimenting\n",
    "#With an explicit figsize here can produce better outcome.\n",
    "fig_totals = plt.figure(linewidth=2,\n",
    "           edgecolor='white', #edge of pdf, not the table\n",
    "           facecolor='white',\n",
    "           tight_layout={'pad':18},\n",
    "           figsize=(10,6)\n",
    "          )\n",
    "# Add a table at the bottom of the axes\n",
    "the_table = plt.table(cellText=cell_text,\n",
    "                      rowLabels=row_headers,\n",
    "                      rowColours=rcolors,\n",
    "                      rowLoc='right',\n",
    "                      colColours=ccolors,\n",
    "                      colLabels=column_headers,\n",
    "                      loc='center')\n",
    "# Scaling is the only influence we have over top and bottom cell padding.\n",
    "# Make the rows taller (i.e., make cell y scale larger).\n",
    "the_table.scale(1, 1.5)\n",
    "# Hide axes\n",
    "ax = plt.gca()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "# Hide axes border\n",
    "plt.box(on=None)\n",
    "# Add title\n",
    "# Add footer\n",
    "#plt.figtext(0.95, 0.05, footer_text, horizontalalignment='right', size=12, weight='light')\n",
    "# Force the figure to update, so backends center objects correctly within the figure.\n",
    "# Without plt.draw() here, the title will center on the axes and not the figure.\n",
    "plt.draw()\n",
    "# Create image. plt.savefig ignores figure edge and face colors, so map them.\n",
    "fig = plt.gcf()\n",
    "plt.savefig('report_directory/weekly_report_'+chosen_date+'/totals_'+chosen_date+'.pdf', bbox_inches='tight', format = 'pdf')\n",
    "# Add title. Not in saved version due to spacing issues\n",
    "plt.suptitle('Totals for metrics '+chosen_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#save all figures as one pdf\n",
    "p = PdfPages('test_file_1.pdf') \n",
    "fig_nums = plt.get_fignums()   \n",
    "figs = [plt.figure(n) for n in fig_nums] \n",
    "for fig in figs:  \n",
    "\n",
    "    # and saving the files \n",
    "    fig.savefig(p, format='pdf')  \n",
    "\n",
    "# close the object \n",
    "p.close() '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# storing metrics\n",
    "Store metrics for future statistical use cases. Cell that creates initial storage file will be commented out and only rerun in case of complete data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save out weekly statistics to separate file with rest of records\n",
    "#eventually will use data to predict relative health of varous table metrics. But first We need to build up those records\n",
    "filename = 'weekly_metric_history_DO_NOT_DELETE.pkl'\n",
    "# Read dictionary pkl file\n",
    "with open(filename, 'rb') as fp:\n",
    "    metrics_history = pickle.load(fp)\n",
    "#record new dates\n",
    "if chosen_date not in metrics_history['date']:\n",
    "    for metric in summed_metric_list:\n",
    "       metrics_history[metric].append(summed_metric_dict[metric][0])\n",
    "    metrics_history['date'].append(chosen_date)\n",
    "# save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(metrics_history, fp)\n",
    "    print('dictionary saved successfully to file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"filename = 'weekly_metric_history_DO_NOT_DELETE.pkl'\\nmetrics_dictionary = {}\\nfor metric in summed_metric_list:\\n    metrics_dictionary[metric] = []\\nmetrics_dictionary['date'] = []\\n\\n# save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\\nwith open(filename, 'wb') as fp:\\n    pickle.dump(metrics_dictionary, fp)\\n    print('dictionary saved successfully to file')\\n    print(metrics_dictionary)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Be careful, as this creates a blank file for the dictionary of metrics. If used foolishly, can delete long term data. Only use if creating new folder or want to reset all data\n",
    "#creat dictionary\n",
    "'''filename = 'weekly_metric_history_DO_NOT_DELETE.pkl'\n",
    "metrics_dictionary = {}\n",
    "for metric in summed_metric_list:\n",
    "    metrics_dictionary[metric] = []\n",
    "metrics_dictionary['date'] = []\n",
    "\n",
    "# save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(metrics_dictionary, fp)\n",
    "    print('dictionary saved successfully to file')\n",
    "    print(metrics_dictionary)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"filename = 'weekly_report_'+chosen_date+'/full_report_'+chosen_date+'.pdf' \\n\\np = PdfPages(filename) \\nfig_master.savefig(p, format='pdf')\\nfig_users.savefig(p, format='pdf')\\nfig_data_flow_rate.savefig(p, format='pdf')\\nfig_totals.savefig(p, format='pdf')\\np.close()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removed feature. plots have vastly different sizing within pdf.\n",
    "#create a single pdf with all figures one after the other\n",
    "'''filename = 'weekly_report_'+chosen_date+'/full_report_'+chosen_date+'.pdf' \n",
    "\n",
    "p = PdfPages(filename) \n",
    "fig_master.savefig(p, format='pdf')\n",
    "fig_users.savefig(p, format='pdf')\n",
    "fig_data_flow_rate.savefig(p, format='pdf')\n",
    "fig_totals.savefig(p, format='pdf')\n",
    "p.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"filename = 'weekly_node_flow_history_DO_NOT_DELETE.pkl'\\nnodes_dictionary = {}\\nfor node in pp_nodes_short:\\n    nodes_dictionary[node] = []\\nnodes_dictionary['date'] = []\\nwith open(filename, 'wb') as fp:\\n    pickle.dump(nodes_dictionary, fp)\\n    print('dictionary saved successfully to file')\\n    print(nodes_dictionary)\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the pkl file for nodes and flow rate\n",
    "'''filename = 'weekly_node_flow_history_DO_NOT_DELETE.pkl'\n",
    "nodes_dictionary = {}\n",
    "for node in pp_nodes_short:\n",
    "    nodes_dictionary[node] = []\n",
    "nodes_dictionary['date'] = []\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(nodes_dictionary, fp)\n",
    "    print('dictionary saved successfully to file')\n",
    "    print(nodes_dictionary)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes dictionary saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "#lastly save data to a pkl file\n",
    "z = 0\n",
    "nodes_dictionary = {}\n",
    "for node in pp_nodes_short:\n",
    "    nodes_dictionary[node] = [flow_avg_pp[z]]\n",
    "    z +=1\n",
    "\n",
    "\n",
    "#eventually will use data to predict relative health of varous nodes. But first We need to build up those records\n",
    "filename = 'weekly_node_flow_history_DO_NOT_DELETE.pkl'\n",
    "# Read dictionary pkl file\n",
    "with open(filename, 'rb') as fp:\n",
    "    nodes_history = pickle.load(fp)\n",
    "    \n",
    "\n",
    "#record new dates\n",
    "if chosen_date not in nodes_history['date']:\n",
    "    for node in pp_nodes_short:\n",
    "        if node != '102' and node != '103':\n",
    "            nodes_history[node].append(nodes_dictionary[node][0])\n",
    "    nodes_history['date'].append(chosen_date)\n",
    "# save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\n",
    "with open(filename, 'wb') as fp:\n",
    "    pickle.dump(nodes_history, fp)\n",
    "    print('nodes dictionary saved successfully to file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
